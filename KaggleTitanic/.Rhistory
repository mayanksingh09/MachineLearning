all[62,]
all[62,]
all[830,]
all$Embarked[c(62,830)]<-"C"
all$Embarked<-as.factor(all$Embarked)
median(all[all$Pclass=='3' & all$Embarked=='S',]$Fare,na.rm = TRUE)
all$Fare[1044]
all$Fare[1044]<-median(all[all$Pclass=='3' & all$Embarked=='S',]$Fare,na.rm = TRUE)
gsub(".*\\ (.*)\\..*","\\1",all$Name)
?gsub
txt <- "a test of capitalizing"
gsub("(\\w)(\\w*)", "\\U\\1\\L\\2", txt, perl=TRUE)
gsub("(\\w)(\\w*)", "\\U\\1\\L\\2", txt, perl=TRUE)
gsub("(\\w)(\\w*)", "\\U\\1\\L\\3", txt, perl=TRUE)
gsub("(\\w)(\\w*)", "\\U\\1\\L\\1", txt, perl=TRUE)
gsub("(\\w)(\\w*)", "\\U\\1\\L\\2", txt, perl=TRUE)
gsub("(\\w)(\\w*)", "\\U\\2\\L\\2", txt, perl=TRUE)
gsub("(\\w)(\\w*)", "\\U\\3\\L\\2", txt, perl=TRUE)
gsub("(\\w)(\\w*)", "\\U\\1\\L\\2", txt, perl=TRUE)
all$title<-gsub(".*\\ (.*)\\..*","\\1",all$Name)
if(FALSE){
all_data$Title[all_data$Title == 'L'] <- 'Other'
all_data$Title[all_data$Title == 'Capt'] <- 'Other'
all_data$Title[all_data$Title == 'Countess'] <- 'Other'
all_data$Title[all_data$Title == 'Don'] <- 'Other'
all_data$Title[all_data$Title == 'Dona'] <- 'Other'
all_data$Title[all_data$Title == 'Mme'] <- 'Other'
all_data$Title[all_data$Title == 'Major'] <- 'Other'
all_data$Title[all_data$Title == 'Jonkheer'] <- 'Other'
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Also reassign mlle, ms, and mme accordingly
all_data$Title[all_data$Title == 'Mlle']        <- 'Miss'
all_data$Title[all_data$Title == 'Ms']          <- 'Miss'
all_data$Title[all_data$Title == 'Mme']         <- 'Mrs'
all_data$Title[all_data$Title %in% rare_title]  <- 'Rare Title'
}
if(FALSE)
{}
all_data$Title[all_data$Title == 'L'] <- 'Other'
all_data$Title[all_data$Title == 'Capt'] <- 'Other'
all_data$Title[all_data$Title == 'Countess'] <- 'Other'
all_data$Title[all_data$Title == 'Don'] <- 'Other'
all_data$Title[all_data$Title == 'Dona'] <- 'Other'
all_data$Title[all_data$Title == 'Mme'] <- 'Other'
all_data$Title[all_data$Title == 'Major'] <- 'Other'
all_data$Title[all_data$Title == 'Jonkheer'] <- 'Other'
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Also reassign mlle, ms, and mme accordingly
all_data$Title[all_data$Title == 'Mlle']        <- 'Miss'
all_data$Title[all_data$Title == 'Ms']          <- 'Miss'
all_data$Title[all_data$Title == 'Mme']         <- 'Mrs'
all_data$Title[all_data$Title %in% rare_title]  <- 'Rare Title'
all$Title[all$Title == 'L'] <- 'Other'
all$Title[all$Title == 'Capt'] <- 'Other'
all$Title[all$Title == 'Countess'] <- 'Other'
all$Title[all$Title == 'Don'] <- 'Other'
all$Title[all$Title == 'Dona'] <- 'Other'
all$Title[all$Title == 'Mme'] <- 'Other'
all$Title[all$Title == 'Major'] <- 'Other'
all$Title[all$Title == 'Jonkheer'] <- 'Other'
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Also reassign mlle, ms, and mme accordingly
all$Title[all$Title == 'Mlle']        <- 'Miss'
all$Title[all$Title == 'Ms']          <- 'Miss'
all$Title[all$Title == 'Mme']         <- 'Mrs'
all$Title[all$Title %in% rare_title]  <- 'Rare Title'
all$title[all$title == 'L'] <- 'Other'
all$title[all$title == 'Capt'] <- 'Other'
all$title[all$title == 'Countess'] <- 'Other'
all$title[all$title == 'Don'] <- 'Other'
all$title[all$title == 'Dona'] <- 'Other'
all$title[all$title == 'Mme'] <- 'Other'
all$title[all$title == 'Major'] <- 'Other'
all$title[all$title == 'Jonkheer'] <- 'Other'
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don',
'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Also reassign mlle, ms, and mme accordingly
all$title[all$title == 'Mlle']        <- 'Miss'
all$title[all$title == 'Ms']          <- 'Miss'
all$title[all$title == 'Mme']         <- 'Mrs'
all$title[all$title %in% rare_title]  <- 'Rare title'
View(all)
all$title<-as.factor(all$title)
all$family_size<-all$Parch + all$SibSp + 1
all$fare_per_person<-all$Fare/all$family_size
View(all)
sapply(all$Name,FUN=function(x){strsplit(x,split = '[,.]')[[1]][1]})
sapply(all$Name,FUN=function(x){strsplit(x,split = '[,.]'))
sapply(all$Name,FUN=function(x){strsplit(x,split = '[,.]')
}
sapply(all$Name,FUN=function(x){strsplit(x,split = '[,.]')[[1]]})
strsplit(all$Name,split = '[,.]')
strsplit(all$Name,split = '[,.]')[1]
strsplit(all$Name,split = '[,.]')[1][1]
strsplit(all$Name,split = '[,.]')[1][1]
strsplit(all$Name,split = '[,.]')[[1][1]
strsplit(all$Name,split = '[,.]')[[1]][1]
all$Surname<-sapply(all$Name,FUN=function(x){strsplit(x,split = '[,.]')[[1]][1]})
library(randomForest)
library(dplyr)
library(rpart)
library(rpart.plot)
library(party)
all$familyID <- paste(as.character(all$family_size), all$surname, sep="")
View(all)
all$familyID[all$family_size < 2] <- 'Small'
famIDs <- data.frame(table(all$familyID))
View(famIDs)
famIDs <- famIDs[famIDs$Freq < 2,]
View(famIDs)
famIDs <- data.frame(table(all$familyID))
famIDs <- famIDs[famIDs$Freq < 2,]
all$familyID[all$familyID %in% famIDs$Var1] <- 'Small'
length(unique(all$familyID))
all$familyID <- factor(all$familyID)
predicted_age <- cforest(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + title + family_size + fare_per_person + familyID,
data = all[!is.na(all$Age),],
controls=cforest_unbiased(ntree=200, mtry=3))
all$Age[is.na(all$Age)] <- predict(predicted_age, all[is.na(all$Age),], OOB=TRUE, type = "response")
all$Child[all$Age < 18] <- 'Child'
all$Child[all$Age >= 18] <- 'Adult'
all$Child <- factor(all$Child)
all$Mother <- 'Not Mother'
all$Deck <- factor(all_data$Deck)
all$Mother[all_data$Sex == 'female' & all_data$Parch > 0 & all_data$Age > 18 & all_data$Title != 'Miss'] <- 'Mother'
all$Mother[all$Sex == 'female' & all$Parch > 0 & all$Age > 18 & all$Title != 'Miss'] <- 'Mother'
all$Mother <- factor(all$Mother)
all$Deck<-substring(all$Cabin, 1, 1)
all$Deck <- factor(all$Deck)
predicted_deck <- cforest(Deck ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title + Child + Mother + family_size + fare_per_person + familyID,
data = all[all$Deck != '',],
controls=cforest_unbiased(ntree=200, mtry=3))
all$Deck[all$Deck == ''] <- predict(predicted_deck, all[all_data$Deck == '',], OOB=TRUE, type = "response")
all$Deck[all$Deck == ''] <- predict(predicted_deck, all[all$Deck == '',], OOB=TRUE, type = "response")
str(all)
train <- all[1:891,]
test <- all[892:1309,]
my_forest <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title
+ Deck + Child + Mother + family_size + fare_per_person + familyID, data = train,
controls=cforest_unbiased(ntree=500, mtry=3))
my_prediction <- predict(my_forest, test, OOB=TRUE, type = "response")
# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
my_solution <- data.frame(PassengerId = test["PassengerId"], Survived = my_prediction)
write.csv(my_solution, file="cforest1.csv",  row.names = FALSE)
clusterdata <- all[, c('Pclass', 'Age', 'SibSp', 'Parch', 'Fare')]
clusterdata$Sex <- as.numeric(all$Sex)
error <- vector()
error[1] <- my_forest$err.rate[2000, c('OOB')]
str(my_forest)
summary(my_forest)
my_forest$err.rate[2000, c('OOB')]
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title + Deck + Child + Mother + Cluster + family_size, data=train, importance = TRUE, ntree=2000)
for(i in 2:15){
cluster <- kmeans(clusterdata, i)
all$Cluster <- as.factor(cluster$cluster)
train <- all[1:891,]
test <- all[892:1309,]
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title + Deck + Child + Mother + Cluster + family_size, data=train, importance = TRUE, ntree=2000)
error[i] <- my_forest$err.rate[2000, c('OOB')]
}
plot(error, type="b")
my_forest$err.rate[2000, c('OOB')]
cluster <- kmeans(clusterdata, 8)
my_forest <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + family_size , data = train,
controls=cforest_unbiased(ntree=500, mtry=3))
library(randomForest)
library(dplyr)
library(rpart)
library(rpart.plot)
library(party)
my_forest <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + family_size , data = train,
controls=cforest_unbiased(ntree=500, mtry=3))
my_prediction <- predict(my_forest, test, OOB=TRUE, type = "response")
# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
my_solution <- data.frame(PassengerId = test["PassengerId"], Survived = my_prediction)
write.csv(my_solution, file="cforest2.csv",  row.names = FALSE)
#result-
library(randomForest)
library(dplyr)
library(rpart)
library(rpart.plot)
library(party)
#Data is used same as TitanicRF2
library(gbm)
library(dplyr)
library(caTools)
library(datasets)
ntrees=5000
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
train_1<-select(train,Pclass , Sex , Age , SibSp , Parch , Fare ,Embarked ,title
,Deck ,Child ,Mother ,family_size ,fare_per_person ,familyID,Cluster)
ntrees=5000
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
Model=gbm.fit(x=all[1:end_trn,]
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 50         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
install.packages("gbm")
detach("package:gbm", unload=TRUE)
remove.packages("gbm", lib="~/R/i686-pc-linux-gnu-library/3.3")
install.packages("gbm")
installed.packages("gbm")
install.packages("gbm")
library("gbm", lib.loc="~/R/i686-pc-linux-gnu-library/3.3")
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
str(train_1$Mother)
View(all)
table(all$Mother)
all$Mother[all$Sex == 'female' & all$Parch > 0 & all$Age > 18 & all$Title != 'Miss'] <- 'Mother'
table(all$Mother)
all$Mother[all$Sex == 'female' & all$Parch > 0 & all$Age > 18 & all$title != 'Miss'] <- 'Mother'
all$Mother<-as.character(all$Mother)
all$Mother[all$Sex == 'female' & all$Parch > 0 & all$Age > 18 & all$title != 'Miss'] <- "Mother"
all$Mother<-as.factor(all$Mother)
table(all$Mother)
predicted_deck <- cforest(Deck ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title + Child + Mother + family_size + fare_per_person + familyID,
data = all[all$Deck != '',],
controls=cforest_unbiased(ntree=200, mtry=3))
library(randomForest)
library(dplyr)
library(rpart)
library(rpart.plot)
library(party)
predicted_deck <- cforest(Deck ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title + Child + Mother + family_size + fare_per_person + familyID,
data = all[all$Deck != '',],
controls=cforest_unbiased(ntree=200, mtry=3))
all$Deck[all$Deck == ''] <- predict(predicted_deck, all[all$Deck == '',], OOB=TRUE, type = "response")
str(all)
train <- all[1:891,]
test <- all[892:1309,]
my_forest <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title
+ Deck + Child + Mother + family_size + fare_per_person + familyID, data = train,
controls=cforest_unbiased(ntree=500, mtry=3))
my_prediction <- predict(my_forest, test, OOB=TRUE, type = "response")
my_solution <- data.frame(PassengerId = test["PassengerId"], Survived = my_prediction)
write.csv(my_solution, file="cforest1.csv",  row.names = FALSE)
clusterdata <- all[, c('Pclass', 'Age', 'SibSp', 'Parch', 'Fare')]
clusterdata$Sex <- as.numeric(all$Sex)
#wss <- (nrow(clusterdata)-1)*sum(apply(clusterdata,2,var))
#  for (i in 2:15) wss[i] <- sum(kmeans(clusterdata, centers=i)$withinss)
#plot(1:15, wss, type="b", xlab="Number of Clusters",
#     ylab="Within groups sum of squares")
error <- vector()
error[1] <- my_forest$err.rate[2000, c('OOB')]
for(i in 2:15){
cluster <- kmeans(clusterdata, i)
all$Cluster <- as.factor(cluster$cluster)
train <- all[1:891,]
test <- all[892:1309,]
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title + Deck + Child + Mother + Cluster + family_size, data=train, importance = TRUE, ntree=2000)
error[i] <- my_forest$err.rate[2000, c('OOB')]
}
plot(error, type="b")
cluster <- kmeans(clusterdata, 8)
all$Cluster <- as.factor(cluster$cluster)
train <- all[1:891,]
test <- all[892:1309,]
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title + Deck + Child + Mother + Cluster + family_size, data=train, importance = TRUE, ntree=4000)
train_1<-select(train,Pclass , Sex , Age , SibSp , Parch , Fare ,Embarked ,title
,Deck ,Child ,Mother ,family_size ,fare_per_person ,familyID,Cluster)
ntrees=5000
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
detach("package:gbm", unload=TRUE)
library("gbm", lib.loc="~/R/i686-pc-linux-gnu-library/3.3")
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
detach("package:gbm", unload=TRUE)
remove.packages("gbm", lib="~/R/i686-pc-linux-gnu-library/3.3")
install.packages("gbm")
install.packages("gbm")
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
library(gbm)
library(dplyr)
library(caTools)
library(datasets)
ntrees=5000
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
Model=gbm.fit(x=all[1:end_trn,]
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 50         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
summary(train_1)
str(train_1)
train_1$Pclass<-as.factor(train_1$Pclass)
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title  + Deck + Child + Mother + family_size + fare_per_person + familyID,data = train,method = "class")
library(randomForest)
library(dplyr)
library(rpart)
library(rpart.plot)
library(party)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title  + Deck + Child + Mother + family_size + fare_per_person + familyID,data = train,method = "class")
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title  + Deck + Child + Mother + family_size + fare_per_person + familyID,data = train,method = "class")
prp(titanicTree)
predictCart<-predict(titanicTree,newdata = test,type = "class")
my_solution <- data.frame(PassengerId = test["PassengerId"], Survived =predictCart)
write.csv(my_solution, file="Cart1.csv",  row.names = FALSE)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked + title  + Deck + Child + Mother + family_size ,data = train,method = "class")
prp(titanicTree)
train$Pclass<-as.factor(train$Pclass)
test$Pclass<-as.factor(test$Pclass)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked + title  + Deck + Child + Mother + family_size ,data = train,method = "class")
prp(titanicTree)
train$family_size<-as.factor(train$family_size)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked + title  + Deck + Child + Mother + family_size ,data = train,method = "class")
prp(titanicTree)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +fare+ title  + Deck + Child + Mother + family_size ,data = train,method = "class")
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +Fare+ title  + Deck + Child + Mother + family_size ,data = train,method = "class")
prp(titanicTree)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +Fare + Deck + Child + Mother + family_size ,data = train,method = "class")
prp(titanicTree)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +Fare + Child + Mother + family_size ,data = train,method = "class")
prp(titanicTree)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +Fare + family_size ,data = train,method = "class")
prp(titanicTree)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +Fare + family_size+Parch ,data = train,method = "class")
prp(titanicTree)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +Fare + family_size+SibSp ,data = train,method = "class")
prp(titanicTree)
str(train$title)
titanicTree<-rpart(as.factor(Survived) ~ Pclass + Sex + Age + Embarked +Fare + family_size,data = train,method = "class")
prp(titanicTree)
predictCart<-predict(titanicTree,newdata = test,type = "class")
my_solution <- data.frame(PassengerId = test["PassengerId"], Survived =predictCart)
test$family_size<-as.factor(test$family_size)
predictCart<-predict(titanicTree,newdata = test,type = "class")
my_solution <- data.frame(PassengerId = test["PassengerId"], Survived =predictCart)
write.csv(my_solution, file="Cart1.csv",  row.names = FALSE)
library(gbm)
library(dplyr)
library(caTools)
library(datasets)
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
str(train)
str(train_1)
train_1$Pclass<-as.factor(as.numeric(train$Pclass))
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
library(caret)
??train
?train
View(train)
tgbm<-train(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title  + Deck + Child + Mother + family_size + fare_per_person + familyID,data = train,method="gbm",trControl=trainControl(method = "cv"))
tgbm<-train(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + title  + Child + Mother + family_size + fare_per_person + familyID,data = train,method="gbm",trControl=trainControl(method = "cv"))
my_prediction <- predict(tgbm, test, type = "response")
my_prediction <- predict(tgbm, test, type = "prob")
my_prediction
Model=gbm.fit(x=all[1:end_trn,]
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 50         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
Model1=gbm.fit(x=train_1
,y=survived
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 20         #small value results in overfitting of data
,nTrain = round(end_trn*0.8)      #use to select number of trees in the end
#,var.monotone=c()
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
my_prediction
my_prediction<-round(my_prediction)
my_prediction
my_prediction[1]
my_prediction[2]
my_prediction[2]
my_prediction
my_prediction[2]
my_solution <- data.frame(PassengerId = test["PassengerId"], Survived = my_prediction[2])
write.csv(my_solution, file="gbm2.csv",  row.names = FALSE)
my_solution
colnames(my_solution)<-c("PassengerId", "Survived")
my_solution
write.csv(my_solution, file="gbm2.csv",  row.names = FALSE)
