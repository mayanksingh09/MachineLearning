{
    "contents" : "#### GBM on complete data set\nlibrary(gbm)\nlibrary(dplyr)\nlibrary(caTools)\n\nimputeData<-imputeCompleteCopy\nimputeData<-select(imputeData,-USER_ID)\n\ntestUserId<-test$USER_ID\ntrainUserId<-train$USER_ID\nimputeTest<-subset(imputeData,imputeData$USER_ID==testUserId)\n\nfor(i in 1:NROW(testUserId))\n{\n  imputeTest[i,]<-imputeData[imputeData$USER_ID==testUserId[i],]\n}\n\nimputeTrain<-subset(imputeData,imputeData$USER_ID==trainUserId)\n\nfor(i in 1:NROW(trainUserId))\n{\n  imputeTrain[i,]<-imputeData[imputeData$USER_ID==trainUserId[i],]\n}\nimputeTrain <- subset(imputeTrain, imputeTrain$YOB <= 2003) \n\ntrainComp<-read.csv(\"train2016.csv\",na.strings = c(\"\",\"NA\"))\nlibrary(stringi)\nlibrary(stringr)\ntrainComp <- subset(trainComp, trainComp$YOB <= 2003) \nimputeTrain$Party<-trainComp$Party\npart<-c(\"Democrat\"=0,\"Republican\"=1)\nimputeTrainGBM<-imputeTrain %>% \n  mutate(Party=str_replace_all(Party,part))\nimputeTrainGBM$Party<-as.factor(imputeTrainGBM$Party)\nParty<-imputeTrainGBM$Party\nimputeTrain<-select(imputeTrain,-Party)\n\n\n#####Running GBM\n\nimputeTrain$Party<-ifelse(imputeTrain$Party==\"Democrat\",TRUE,FALSE)\nParty<-imputeTrain$Party\nimputeTrain<-select(imputeTrain,-Party)\n\n\nntrees=5000\nrow_train<-nrow(imputeTrain)\nKaggleGBM=gbm.fit(x=imputeTrain\n              ,y=Party\n              ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n              ,n.trees = ntrees             #select large value for tree and then later on prune it\n              ,shrinkage = 0.01             #set smaller value for shrinkaggive better performance\n              #but it may result in taking a longer time to execute\n              ,interaction.depth = 3        #use cross validation to choose interaction depth\n              ,n.minobsinnode = 10          #small value results in overfitting of data\n              ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n              #,var.monotone=c()\n              #can help to smoothen the curves and help with overfitting            \n              ,verbose=TRUE)                #print the output in the end\n\n\nsummary(KaggleGBM)\ngbm.perf(KaggleGBM)\n\nfor(i in 1:length(KaggleGBM$var.names))\n{\n  plot(KaggleGBM,i.var=i\n       ,ntree=gbm.perf(KaggleGBM,plot.it = FALSE),type=\"response\")\n}\n\n\n#make predictionns\n\n#test set predictions\nTestPredictions<-predict(object = KaggleGBM,newdata = test\n                         ,n.trees = gbm.perf(KaggleGBM,plot.it = FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions<-predict(object = KaggleGBM,newdata = train\n                          ,n.trees = gbm.perf(KaggleGBM,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions<-round(TestPredictions)\nTrainPredictions<-round(TrainPredictions)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions<-ifelse(TrainPredictions==1,\"Democrat\",\"Republican\")\nTrainPredictions<-as.factor(TrainPredictions)\ntable(trainComp$Party,TrainPredictions)\n(2194+1235)/nrow(trainComp)\n\nTestPredictions<-ifelse(TestPredictions==1,\"Democrat\",\"Republican\")\nTestPredictions<-as.factor(TestPredictions)\ntable(TestPredictions)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions)\nwrite.csv(MySubmission, \"SubmissionGBMwithCompleteImputed.csv\", row.names=FALSE)\n#result 0.61 \n\n\n\n#gbm 2 GBM on complete training set\n\ntrain$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nParty<-train$Party\ntrain<-select(train,-Party)\n\nntrees=5000\nrow_train<-nrow(train)\nKaggleGBM1=gbm.fit(x=train\n                  ,y=Party\n                  ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n                  ,n.trees = ntrees             #select large value for tree and then later on prune it\n                  ,shrinkage = 0.01             #set smaller value for shrinkaggive better performance\n                  #but it may result in taking a longer time to execute\n                  ,interaction.depth = 3        #use cross validation to choose interaction depth\n                  ,n.minobsinnode = 10          #small value results in overfitting of data\n                  ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n                  #,var.monotone=c()\n                  #can help to smoothen the curves and help with overfitting            \n                  ,verbose=TRUE)                #print the output in the end\n\nsummary(KaggleGBM1)\ngbm.perf(KaggleGBM1)\n\nfor(i in 1:length(KaggleGBM1$var.names))\n{\n  plot(KaggleGBM1,i.var=i\n       ,ntree=gbm.perf(KaggleGBM1,plot.it = FALSE),type=\"response\")\n}\n\n#make predictionns\n\n#test set predictions\nTestPredictions<-predict(object = KaggleGBM1,newdata = test\n                         ,n.trees = gbm.perf(KaggleGBM1,plot.it = FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions<-predict(object = KaggleGBM1,newdata = train\n                          ,n.trees = gbm.perf(KaggleGBM1,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions<-round(TestPredictions)\nTrainPredictions<-round(TrainPredictions)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions<-ifelse(TrainPredictions==1,\"Democrat\",\"Republican\")\nTrainPredictions<-as.factor(TrainPredictions)\ntable(trainComp$Party,TrainPredictions)\n(2067+1695)/nrow(trainComp)\n\nTestPredictions<-ifelse(TestPredictions==1,\"Democrat\",\"Republican\")\nTestPredictions<-as.factor(TestPredictions)\ntable(TestPredictions)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions)\nwrite.csv(MySubmission, \"SubmissionGBMwithOriginalTraining.csv\", row.names=FALSE)\n#result 0.64 \n\n\n################################################################################\n#GBM 3rd version with imp questions---------------------------------------------\n\nsummary(KaggleGBM1)\ngbm.perf(KaggleGBM1)\n\nfor(i in 1:length(KaggleGBM1$var.names))\n{\n  plot(KaggleGBM1,i.var=i\n       ,ntree=gbm.perf(KaggleGBM1,plot.it = FALSE),type=\"response\")\n}\n\nall<-select(train,Q109244,EducationLevel,Income,HouseholdStatus,YOB,Q115611,Q98197,Q113181,Q101163,Q118232,Q98869,Q110740,Gender,Q120379,Q112478,Q101596,Q123621,Q116953,Q120194,Q105655,Q108855,Q104996,Q106997,Q100689,Q119851,Q116881)\nalltest<-select(test,Q109244,EducationLevel,Income,HouseholdStatus,YOB,Q115611,Q98197,Q113181,Q101163,Q118232,Q98869,Q110740,Gender,Q120379,Q112478,Q101596,Q123621,Q116953,Q120194,Q105655,Q108855,Q104996,Q106997,Q100689,Q119851,Q116881)\nntrees=5000\nrow_train<-nrow(all)\nKaggleGBM2=gbm.fit(x=all\n                   ,y=Party\n                   ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n                   ,n.trees = ntrees             #select large value for tree and then later on prune it\n                   ,shrinkage = 0.01             #set smaller value for shrinkag give better performance\n                   #but it may result in taking a longer time to execute\n                   ,interaction.depth = 3        #use cross validation to choose interaction depth\n                   ,n.minobsinnode = 10          #small value results in overfitting of data\n                   ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n                   #,var.monotone=c()\n                   #can help to smoothen the curves and help with overfitting            \n                   ,verbose=TRUE)                #print the output in the end\n\nsummary(KaggleGBM2)\ngbm.perf(KaggleGBM2)\n\nfor(i in 1:length(KaggleGBM2$var.names))\n{\n  plot(KaggleGBM2,i.var=i\n       ,ntree=gbm.perf(KaggleGBM2,plot.it = FALSE),type=\"response\")\n}\n\n#make prediction\n\n#test set predictions\nTestPredictions2<-predict(object = KaggleGBM2,newdata = alltest\n                         ,n.trees = gbm.perf(KaggleGBM2,plot.it = FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions2<-predict(object = KaggleGBM2,newdata = all\n                          ,n.trees = gbm.perf(KaggleGBM2,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions2<-round(TestPredictions2)\nTrainPredictions2<-round(TrainPredictions2)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions2<-ifelse(TrainPredictions2==1,\"Democrat\",\"Republican\")\nTrainPredictions2<-as.factor(TrainPredictions2)\ntable(trainComp$Party,TrainPredictions2)\n(2022+1482)/nrow(trainComp)\n\nTestPredictions2<-ifelse(TestPredictions2==1,\"Democrat\",\"Republican\")\nTestPredictions2<-as.factor(TestPredictions2)\ntable(TestPredictions2)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions2)\nwrite.csv(MySubmission, \"SubmissionGBMwithOriginalTrainingAndLessVariables.csv\", row.names=FALSE)\n\n\n\n\n\n\n#Q109244,EducationLevel,Income,HouseholdStatus,YOB,Q115611,Q98197,Q113181,Q101163,Q118232,Q98869,Q110740,Gender,Q120379,Q112478,Q101596,Q123621,Q116953,Q120194,Q105655,Q108855,Q104996,Q106997,Q100689,Q119851,Q116881                 \n\n\n############################################################################################\n#gbm 4----------------------------------------- \n#GBM on complete training set with larger value of minobisnode to avoid overfitting\n\ntrain$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nParty<-train$Party\ntrain<-select(train,-Party)\n#minobsinnode value =20-0.6947 on training set,50-0.6866,100-0.69,30-0.696613,25-0.7012044,original entry value =10 \nntrees=5000\nrow_train<-nrow(train)\nKaggleGBM3=gbm.fit(x=train\n                   ,y=Party\n                   ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n                   ,n.trees = ntrees             #select large value for tree and then later on prune it\n                   ,shrinkage = 0.01             #set smaller value for shrinkaggive better performance\n                   #but it may result in taking a longer time to execute\n                   ,interaction.depth = 3        #use cross validation to choose interaction depth\n                   ,n.minobsinnode = 25         #small value results in overfitting of data\n                   ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n                   #,var.monotone=c()\n                   #can help to smoothen the curves and help with overfitting            \n                   ,verbose=TRUE)                #print the output in the end\n\nsummary(KaggleGBM3)\ngbm.perf(KaggleGBM3)\nbest.iter <- gbm.perf(KaggleGBM3,method=\"OOB\")\nprint(best.iter)\n\nfor(i in 1:length(KaggleGBM3$var.names))\n{\n  plot(KaggleGBM3,i.var=i\n       ,ntree=gbm.perf(KaggleGBM3,plot.it = FALSE),type=\"response\")\n}\n\n#make predictionns\n\n#test set predictions\nTestPredictions<-predict(object = KaggleGBM3,newdata = test\n                         ,n.trees = gbm.perf(KaggleGBM3,plot.it = FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions<-predict(object = KaggleGBM3,newdata = train\n                          ,n.trees = gbm.perf(KaggleGBM3,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions<-round(TestPredictions)\nTrainPredictions<-round(TrainPredictions)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions<-ifelse(TrainPredictions==1,\"Democrat\",\"Republican\")\nTrainPredictions<-as.factor(TrainPredictions)\ntable(trainComp$Party,TrainPredictions)\n(2087+1673)/nrow(trainComp)\n\nTestPredictions<-ifelse(TestPredictions==1,\"Democrat\",\"Republican\")\nTestPredictions<-as.factor(TestPredictions)\ntable(TestPredictions)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions)\nwrite.csv(MySubmission, \"SubmissionGBMwithOriginalTrainingAndUnderFitting2.csv\", row.names=FALSE)\n#result for minobsinnode value=30 -  0.65661 and 25-0.6545\n\n\n############################################################################################\n#gbm 5----------------------------------------- \n#GBM on complete training set without user ID\n\ntrain$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nParty<-train$Party\ntrain<-select(train,-Party)\ntrainNew<-select(train,-USER_ID)\ntestNew<-select(test,-USER_ID)\n#minobsinnode value =20-0.7 on training set,50-0.6866,100-0.69,30-0.69,15-original entry value =10 \n#shrinkage value - original - 0.01 current 0.001 - \nntrees=20000\nrow_train<-nrow(trainNew)\nKaggleGBM4=gbm.fit(x=trainNew\n                   ,y=Party\n                   ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n                   ,n.trees = ntrees             #select large value for tree and then later on prune it\n                   ,shrinkage = 0.001             #set smaller value for shrinkaggive better performance\n                   #but it may result in taking a longer time to execute\n                   ,interaction.depth = 3        #use cross validation to choose interaction depth\n                   ,n.minobsinnode = 30   #small value results in overfitting of data\n                   #,cv.folds=3\n                   ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n                   #,var.monotone=c(-1)\n                   #can help to smoothen the curves and help with overfitting   \n                  \n                   ,verbose=TRUE)                #print the output in the end\n\nsummary(KaggleGBM4)\ngbm.perf(KaggleGBM4)\n\nfor(i in 1:length(KaggleGBM4$var.names))\n{\n  plot(KaggleGBM3,i.var=i\n       ,ntree=gbm.perf(KaggleGBM4,plot.it = FALSE),type=\"response\")\n}\n\n#make predictionns\n\n#test set predictions\nTestPredictions<-predict(object = KaggleGBM4,newdata = testNew\n                         ,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions<-predict(object = KaggleGBM4,newdata = trainNew\n                          ,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions<-round(TestPredictions)\nTrainPredictions<-round(TrainPredictions)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions<-ifelse(TrainPredictions==1,\"Democrat\",\"Republican\")\nTrainPredictions<-as.factor(TrainPredictions)\ntable(trainComp$Party,TrainPredictions)\n(2102+1615)/nrow(trainComp)\n\nTestPredictions<-ifelse(TestPredictions==1,\"Democrat\",\"Republican\")\nTestPredictions<-as.factor(TestPredictions)\ntable(TestPredictions)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions)\nwrite.csv(MySubmission, \"SubmissionGBMwithOriginalTrainingAndUnderFittingAndTREES.csv\", row.names=FALSE)\n#result for ntrees=20000, 0.66\n\n\n##################################################################################################\n#GBM 6--------------------------\n#with selected variables without imputations\n\n\ntrain$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nParty<-train$Party\ntrain<-select(train,-Party)\ntrainNew1<-select(trainNew,Q109244,Q115611,Q118232,Q110740,Q120379,Q118237,Q121011,Q123621,Q123464,Q115195,Q98197,Q120472,Q121011,Q118117,Q119334,Q111848,Q121699,Q101163,Q116881,Q116953,Q115899,Q122771,YOB,Gender,Income,HouseholdStatus,EducationLevel)\ntestNew1<-select(testNew,Q109244,Q115611,Q118232,Q110740,Q120379,Q118237,Q121011,Q123621,Q123464,Q115195,Q98197,Q120472,Q121011,Q118117,Q119334,Q111848,Q121699,Q101163,Q116881,Q116953,Q115899,Q122771,YOB,Gender,Income,HouseholdStatus,EducationLevel)\n\n#minobsinnode value =20-0.7 on training set,50-0.6866,100-0.69,30-0.69,15-original entry value =10 \n#shrinkage value - original - 0.01 current 0.001 - \nntrees=5000\nrow_train<-nrow(trainNew1)\nKaggleGBM5=gbm.fit(x=trainNew1\n                   ,y=Party\n                   ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n                   ,n.trees = ntrees             #select large value for tree and then later on prune it\n                   ,shrinkage = 0.01             #set smaller value for shrinkaggive better performance\n                   #but it may result in taking a longer time to execute\n                   ,interaction.depth = 3        #use cross validation to choose interaction depth\n                   ,n.minobsinnode = 25   #small value results in overfitting of data\n                   #,cv.folds=3\n                   ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n                   #,var.monotone=c(-1)\n                   #can help to smoothen the curves and help with overfitting   \n                   \n                   ,verbose=TRUE)                #print the output in the end\n\nsummary(KaggleGBM5)\ngbm.perf(KaggleGBM5)\n\nfor(i in 1:length(KaggleGBM5$var.names))\n{\n  plot(KaggleGBM5,i.var=i\n       ,ntree=gbm.perf(KaggleGBM5,plot.it = FALSE),type=\"response\")\n}\n\n#make predictionns\n\n#test set predictions\nTestPredictions<-predict(object = KaggleGBM5,newdata = testNew1\n                         ,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1\n                          ,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions<-round(TestPredictions)\nTrainPredictions<-round(TrainPredictions)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions<-ifelse(TrainPredictions==1,\"Democrat\",\"Republican\")\nTrainPredictions<-as.factor(TrainPredictions)\ntable(trainComp$Party,TrainPredictions)\n(2013+1594)/nrow(trainComp)\n\nTestPredictions<-ifelse(TestPredictions==1,\"Democrat\",\"Republican\")\nTestPredictions<-as.factor(TestPredictions)\ntable(TestPredictions)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions)\nwrite.csv(MySubmission, \"SubmissionGBMwithOriginalTrainingAndUnderFittingAndTREES.csv\", row.names=FALSE)\n#result for ntrees=20000, 0.66\n\n\n############################################################################################\n#gbm 7----------------------------------------- \n#GBM on complete training set with demography imputed\n\ntrain$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nParty<-train$Party\ntrain<-select(train,-Party)\ntrainNew2<-select(train,-USER_ID)\ntestNew2<-select(test,-USER_ID)\ntrainNew2[,c(1:5)]<-imputeTrain[,c(2:6)]\ntestNew2[,c(1:5)]<-imputeTest[,c(2:6)]\ntrainNew2<-select(train,Gender,Q102089,Q106272,Q109244,Q113181,Q115611,Q115899,Q116881,Q116953,Q118232,Q121699,Q98869)\ntestNew2<-select(test,Gender,Q102089,Q106272,Q109244,Q113181,Q115611,Q115899,Q116881,Q116953,Q118232,Q121699,Q98869)\n#minobsinnode value =20-0.7 on training set,50-0.6866,100-0.69,30-0.69,15-original entry value =10 \n#shrinkage value - original - 0.01 current 0.001 - \nntrees=20000\nrow_train<-nrow(trainNew2)\nKaggleGBM4=gbm.fit(x=trainNew2\n                   ,y=Party\n                   ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n                   ,n.trees = ntrees             #select large value for tree and then later on prune it\n                   ,shrinkage = 0.001             #set smaller value for shrinkaggive better performance\n                   #but it may result in taking a longer time to execute\n                   ,interaction.depth = 3        #use cross validation to choose interaction depth\n                   ,n.minobsinnode = 30   #small value results in overfitting of data\n                   #,cv.folds=3\n                   ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n                   #,var.monotone=c(-1)\n                   #can help to smoothen the curves and help with overfitting   \n                   \n                   ,verbose=TRUE)                #print the output in the end\n\nsummary(KaggleGBM4)\ngbm.perf(KaggleGBM4)\n\nfor(i in 1:length(KaggleGBM4$var.names))\n{\n  plot(KaggleGBM4,i.var=i\n       ,ntree=gbm.perf(KaggleGBM4,plot.it = FALSE),type=\"response\")\n}\n\n#make predictionns\n\n#test set predictions\nTestPredictions<-predict(object = KaggleGBM4,newdata = testNew2\n                         ,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions<-predict(object = KaggleGBM4,newdata = trainNew2\n                          ,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions<-round(TestPredictions)\nTrainPredictions<-round(TrainPredictions)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions<-ifelse(TrainPredictions==1,\"Democrat\",\"Republican\")\nTrainPredictions<-as.factor(TrainPredictions)\ntable(trainComp$Party,TrainPredictions)\n(2102+1615)/nrow(trainComp)\n\nTestPredictions<-ifelse(TestPredictions==1,\"Democrat\",\"Republican\")\nTestPredictions<-as.factor(TestPredictions)\ntable(TestPredictions)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions)\nwrite.csv(MySubmission, \"SubmissionGBMwithOriginalTrainingAndUnderFittingAndTREES.csv\", row.names=FALSE)\n#result for ntrees=20000, \n\n\n############################################################################################\n#gbm 8----------------------------------------- \n#GBM on complete training set with everything imputed\n\ntrain$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nParty<-train$Party\ntrain<-select(train,-Party)\nimputeTrain<-select(imputeTrain,-USER_ID)\nimputeTest<-select(imputeTest,-USER_ID)\n#trainNew2<-select(train,Gender,Q102089,Q106272,Q109244,Q113181,Q115611,Q115899,Q116881,Q116953,Q118232,Q121699,Q98869)\n#testNew2<-select(test,Gender,Q102089,Q106272,Q109244,Q113181,Q115611,Q115899,Q116881,Q116953,Q118232,Q121699,Q98869)\n#minobsinnode value =20-0.7 on training set,50-0.6866,100-0.69,30-0.69,15-original entry value =10 \n#shrinkage value - original - 0.01 current 0.001 - \nntrees=20000\nrow_train<-nrow(imputeTrain)\nKaggleGBM4=gbm.fit(x=imputeTrain\n                   ,y=Party\n                   ,distribution = \"bernoulli\"   #bernoulli for classification and gaussian for regression/adaboost\n                   ,n.trees = ntrees             #select large value for tree and then later on prune it\n                   ,shrinkage = 0.001             #set smaller value for shrinkaggive better performance\n                   #but it may result in taking a longer time to execute\n                   ,interaction.depth = 3        #use cross validation to choose interaction depth\n                   ,n.minobsinnode = 30   #small value results in overfitting of data\n                   #,cv.folds=3\n                   ,nTrain = round(row_train*0.8)      #use to select number of trees in the end\n                   #,var.monotone=c(-1)\n                   #can help to smoothen the curves and help with overfitting   \n                   \n                   ,verbose=TRUE)                #print the output in the end\n\nsummary(KaggleGBM4)\ngbm.perf(KaggleGBM4)\n\nfor(i in 1:length(KaggleGBM4$var.names))\n{\n  plot(KaggleGBM4,i.var=i\n       ,ntree=gbm.perf(KaggleGBM4,plot.it = FALSE),type=\"response\")\n}\n\n#make predictionns\n\n#test set predictions\nTestPredictions<-predict(object = KaggleGBM4,newdata = imputeTest\n                         ,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),\n                         type = \"response\")\n#training set predictions\nTrainPredictions<-predict(object = KaggleGBM4,newdata = imputeTrain\n                          ,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),\n                          type = \"response\")\nTestPredictions<-round(TestPredictions)\nTrainPredictions<-round(TrainPredictions)\n\n#train$Party<-ifelse(train$Party==\"Democrat\",TRUE,FALSE)\nTrainPredictions<-ifelse(TrainPredictions==1,\"Democrat\",\"Republican\")\nTrainPredictions<-as.factor(TrainPredictions)\ntable(trainComp$Party,TrainPredictions)\n(2102+1615)/nrow(trainComp)\n\nTestPredictions<-ifelse(TestPredictions==1,\"Democrat\",\"Republican\")\nTestPredictions<-as.factor(TestPredictions)\ntable(TestPredictions)\nMySubmission = data.frame(USER_ID = test$USER_ID, Predictions = TestPredictions)\nwrite.csv(MySubmission, \"SubmissionGBMwithOriginalTrainingAndUnderFittingAndTREES.csv\", row.names=FALSE)\n#result for ntrees=20000, 0.66\n\n\n",
    "created" : 1465636566745.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3610247046",
    "id" : "9DA48D84",
    "lastKnownWriteTime" : 1465844032,
    "path" : "~/Documents/KaggleAnalytisEdge/KaggleGBM.R",
    "project_path" : "KaggleGBM.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_source"
}