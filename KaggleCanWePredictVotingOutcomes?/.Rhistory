,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.01             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 10   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
summary(KaggleGBM5)
trainNew1<-select(trainNew,Q109244,Q115611,Q118232,Q98197,YOB,Gender,Income,HouseholdStatus,EducationLevel)
testNew1<-select(testNew,Q109244,Q115611,Q118232,Q98197,YOB,Gender,Income,HouseholdStatus,EducationLevel)
ntrees=5000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.01             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 10   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
gbm.perf(KaggleGBM5)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ntrees=20000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
gbm.perf(KaggleGBM5)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ntrees=20000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 50   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
gbm.perf(KaggleGBM5)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ntrees=15000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 15   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
gbm.perf(KaggleGBM5)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ntrees=15000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 7   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
TestPredictions<-predict(object = KaggleGBM4,newdata = testNew
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM4,newdata = trainNew
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ntrees=20000
row_train<-nrow(trainNew)
KaggleGBM4=gbm.fit(x=trainNew
,y=Party
,distribution = "adaboost"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
summary(KaggleGBM4)
gbm.perf(KaggleGBM4)
TestPredictions<-predict(object = KaggleGBM4,newdata = testNew
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM4,newdata = trainNew
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
trainNew1<-select(trainNew,Q109244,Q115611,Q118232,Q110740,Q120379,Q118237,Q121011,Q123621,Q123464,Q115195,Q98197,Q120472,Q121011,Q118117,Q119334,Q111848,Q121699,Q101163,Q116881,Q116953,Q115899,Q122771,YOB,Gender,Income,HouseholdStatus,EducationLevel)
testNew1<-select(testNew,Q109244,Q115611,Q118232,Q110740,Q120379,Q118237,Q121011,Q123621,Q123464,Q115195,Q98197,Q120472,Q121011,Q118117,Q119334,Q111848,Q121699,Q101163,Q116881,Q116953,Q115899,Q122771,YOB,Gender,Income,HouseholdStatus,EducationLevel)
ntrees=20000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
gbm.perf(KaggleGBM5)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
(1999+1558)/nrow(trainComp)
ntrees=5000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.01             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
summary(KaggleGBM5)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
(2013+1594)/nrow(trainComp)
ntrees=5000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.01             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 50   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ntrees=5000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.01             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 37   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
gbm.perf(KaggleGBM5)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ntrees=5000
row_train<-nrow(trainNew1)
KaggleGBM5=gbm.fit(x=trainNew1
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.01             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 25   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
TestPredictions<-predict(object = KaggleGBM5,newdata = testNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it =  FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM5,newdata = trainNew1
,n.trees = gbm.perf(KaggleGBM5,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
View(imputeNewCopy)
View(imputeTrainNew)
trainNew2<-select(train,-USER_ID)
testNew2<-select(test,-USER_ID)
testnew2[,2]
trainNew2<-select(train,-USER_ID)
testNew2<-select(test,-USER_ID)
testNew2[,2]
View(testNew2)
testNew2[,c(1:5)]
trainNew2[,c(1:5)]<-imputeTrainNew[,c(2:6)]
testNew2[,c(1:5)]
View(trainNew2)
trainNew2[,c(1:5)]
View(imputeTestNew)
testNew2[,c(1:5)]<-imputeTestNew[,c(2:6)]
testNew2[,c(1:5)]
ntrees=20000
row_train<-nrow(trainNew2)
KaggleGBM4=gbm.fit(x=trainNew2
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)
gbm.perf(KaggleGBM4)
TestPredictions<-predict(object = KaggleGBM4,newdata = testNew2
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM4,newdata = trainNew2
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
library(gbm)
library(dplyr)
library(caTools)
View(imputeCompleteCopy)
trainNew2[,c(1:5)]<-imputeCompleteCopy[,c(2:6)]
View(imputeCompleteCopy)
View(imputeTrain)
trainNew2[,c(1:5)]<-imputeTrain[,c(2:6)]
testNew2[,c(1:5)]<-imputeTest[,c(2:6)]
ntrees=20000
row_train<-nrow(trainNew2)
KaggleGBM4=gbm.fit(x=trainNew2
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
gbm.perf(KaggleGBM4)
TestPredictions<-predict(object = KaggleGBM4,newdata = testNew2
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM4,newdata = trainNew2
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
ibrary(gbm)
library(dplyr)
library(caTools)
library(gbm)
trainNew2<-select(train,Gender,Q102089,Q106272,Q109244,Q113181,Q115611,Q115899,Q116881,Q116953,Q118232,Q121699,Q98869)
testNew2<-select(test,Gender,Q102089,Q106272,Q109244,Q113181,Q115611,Q115899,Q116881,Q116953,Q118232,Q121699,Q98869)
ntrees=20000
row_train<-nrow(trainNew2)
KaggleGBM4=gbm.fit(x=trainNew2
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
gbm.perf(KaggleGBM4)
TestPredictions<-predict(object = KaggleGBM4,newdata = testNew2
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM4,newdata = trainNew2
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
imputeTrain<-select(imputeTrain,-USER_ID)
imputeTest<-select(imputeTest,-USER_ID)
ntrees=20000
row_train<-nrow(imputeTrain)
KaggleGBM4=gbm.fit(x=imputeTrain
,y=Party
,distribution = "bernoulli"   #bernoulli for classification and gaussian for regression/adaboost
,n.trees = ntrees             #select large value for tree and then later on prune it
,shrinkage = 0.001             #set smaller value for shrinkaggive better performance
#but it may result in taking a longer time to execute
,interaction.depth = 3        #use cross validation to choose interaction depth
,n.minobsinnode = 30   #small value results in overfitting of data
#,cv.folds=3
,nTrain = round(row_train*0.8)      #use to select number of trees in the end
#,var.monotone=c(-1)
#can help to smoothen the curves and help with overfitting
,verbose=TRUE)                #print the output in the end
gbm.perf(KaggleGBM4)
TestPredictions<-predict(object = KaggleGBM4,newdata = imputeTest
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
#training set predictions
TrainPredictions<-predict(object = KaggleGBM4,newdata = imputeTrain
,n.trees = gbm.perf(KaggleGBM4,plot.it = FALSE),
type = "response")
TestPredictions<-round(TestPredictions)
TrainPredictions<-round(TrainPredictions)
#train$Party<-ifelse(train$Party=="Democrat",TRUE,FALSE)
TrainPredictions<-ifelse(TrainPredictions==1,"Democrat","Republican")
TrainPredictions<-as.factor(TrainPredictions)
table(trainComp$Party,TrainPredictions)
train <- read.csv("~/Documents/Titanic/train.csv")
View(train)
